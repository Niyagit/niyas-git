# -*- coding: utf-8 -*-
"""Breast-cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-JSYEkRRgCXpbjqeAfSiK00duh6XtxTh
"""

from google.colab import files


uploaded = files.upload()

# import regular libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# import models from Scikit-learn
from sklearn.linear_model import LogisticRegression 
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# import model evaluation tools
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import plot_roc_curve

# import data
from sklearn.datasets import load_breast_cancer

# save and view the data set 
data = load_breast_cancer()

data

# let‘s see the keys
data.keys()

# let‘s look at DESCR key‘s value
data.DESCR

# Turn the feature data into a dataframe
df = pd.DataFrame(data.data, 
                  columns = data.feature_names)

# Add the target columns, and fill it with the target data
df["target"] = data.target

# Show the dataframe
df

# See the dataframe information
df.info()

df.isna().sum()

# see the target value counts
df["target"].value_counts()

# plot value counts
df["target"].value_counts().plot(kind="bar", color=["peru", "darkmagenta"]);

# Make the correlation matrix 
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(10, 7))
ax = sns.heatmap(corr_matrix)

# store data in X and y variables
X = data.data
y = data.target

# Split the data using Scikit-Learn‘s train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=0)

# Create a dictionary of machine learning algorithms

models = {"Logistic regression": LogisticRegression(),
          "KNN": KNeighborsClassifier(),
          "Linear SVC": LinearSVC(),
          "Random Forest": RandomForestClassifier()}

 # Create a funtion to train (fit) and score models
def train_score(models, X_train, X_test, y_train, y_test):
  # Set random seed
  np.random.seed(0)
  # Make an empty dictionary for model scores
  scores = {}  
  # Loop through models
  for name, model in models.items():
      #Fit the model to the data
      model.fit(X_train, y_train)
      #Evaluate the model and append its score to the scores dictionary
      scores[name] = model.score(X_test, y_test)
  return scores

# let‘s try the function
scores = train_score(models = models,
                   X_train = X_train,
                   X_test = X_test,
                   y_train = y_train,
                   y_test = y_test)

scores

# Model comparison
model_compare = pd.DataFrame(scores, index=["accuracy"])
model_compare.T.plot.bar();

# Hyperparameter grid for RandomForest model
rf_grid = {"n_estimators":np.arange(10, 1000, 20),
           "max_depth":[None, 3, 5, 10],
           "max_features":["sqrt", "log2", "auto"],
           "min_samples_split":np.arange(2, 20, 2),
           "min_samples_leaf":np.arange(1, 20, 2)}

#Setup random hyperparameter search for RandomForest Classifier
random_rf = RandomizedSearchCV(RandomForestClassifier(),
                               param_distributions=rf_grid,
                               cv=5,
                               n_iter=20,
                               n_jobs=-1,
                               random_state=0)
  
# Fit random hyperparameter model
random_rf.fit(X_train, y_train)


print(f"Train score: {random_rf.score(X_train, y_train)}")
print(f"Test score: {random_rf.score(X_test, y_test)}")

# Hyperparameter grid for Logistic Regression
log_reg_grid = {"C" : np.logspace(-4, 4, 20),
               "solver":["liblinear"]}

# Tune Logistic Regression

random_logr = RandomizedSearchCV(LogisticRegression(),
                               param_distributions=log_reg_grid,
                               cv=5,
                               n_iter=20,
                               random_state=0)

#Fit random hyperparameter search model for Logistic Regression
random_logr.fit(X_train, y_train)

print(f"Train score: {random_logr.score(X_train, y_train)}")
print(f"Test score: {random_logr.score(X_test, y_test)}")

random_logr.best_params_

df.head()

# import Standard Scaler
from sklearn.preprocessing import StandardScaler

# instantiate Standard Scaler
sc = StandardScaler()

# fit on our train data
sc.fit(X_train)

# transform ans store scaled train and test data in new variables
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)

# original train data array
X_train

# scaled train data array
X_train_std

# make a dictionary of our models
models = {"Logistic regression": LogisticRegression(),
          "KNN": KNeighborsClassifier(),
          "Random Forest": RandomForestClassifier()}

# use the function for model fitting and socring that we created earlier
scores_scaled = train_score(models = models,
                   X_train = X_train_std,
                   X_test = X_test_std,
                   y_train = y_train,
                   y_test = y_test)

scores_scaled

scores

#Fit random hyperparameter search model for Logistic Regression
random_logr.fit(X_train_std, y_train)

print(f"Train score: {random_logr.score(X_train_std, y_train)}")
print(f"Test score: {random_logr.score(X_test_std, y_test)}")

# Check best parameters
random_logr.best_params_

# Use parameters as the best model
best_logr = LogisticRegression()
best_logr.set_params(**random_logr.best_params_)

# Fit best model
best_logr.fit(X_train_std, y_train)

#Score best model
best_logr.score(X_test_std, y_test)

# Fit random hyperparameter model for Random Forest
random_rf.fit(X_train_std, y_train)


print(f"Train score: {random_rf.score(X_train_std, y_train)}")
print(f"Test score: {random_rf.score(X_test_std, y_test)}")

# Check best parameters
random_rf.best_params_

# Use parameters for the best model
best_rf = RandomForestClassifier()
best_rf.set_params(**random_rf.best_params_)

# Fit best model
best_rf.fit(X_train_std, y_train)

# Score best model
best_rf.score(X_test_std, y_test)

# make a grid for knn

knn_grid = {"n_neighbors" : np.arange(1,21,2),
            "weights" : ["uniform", "distance"],
            "metric" : ["euclidean", "manhattan", "minkowski"]}


#Setup random hyperparameter search for KNN

knn = RandomizedSearchCV(KNeighborsClassifier(),
                         param_distributions=knn_grid,
                         cv=5,
                         n_iter=20,
                         n_jobs=-1,
                         random_state=0)
  
# Fit random hyperparameter model
knn.fit(X_train_std, y_train)


print(f"Train score: {random_rf.score(X_train_std, y_train)}")
print(f"Test score: {random_rf.score(X_test_std, y_test)}")

# make a grid for knn

knn_grid = {"n_neighbors" : np.arange(1,21,2),
            "weights" : ["uniform", "distance"],
            "metric" : ["euclidean", "manhattan", "minkowski"]}


#Setup random hyperparameter search for KNN

knn = RandomizedSearchCV(KNeighborsClassifier(),
                         param_distributions=knn_grid,
                         cv=5,
                         n_iter=20,
                         n_jobs=-1,
                         random_state=0)
  
# Fit random hyperparameter model
knn.fit(X_train_std, y_train)

# Use parameters for the best model
best_knn = KNeighborsClassifier()
best_knn.set_params(**knn.best_params_)

# Fit best model
best_knn.fit(X_train_std, y_train)

# Score best model
best_knn.score(X_test_std, y_test)

# make predictions
y_preds_rf = best_rf.predict(X_test_std)
y_preds_lr = best_logr.predict(X_test_std)
y_preds_knn = best_knn.predict(X_test_std)

# create a dictionary of preds and true labels
dict = {"Random Forest": {"predictions": y_preds_rf, "true labels": y_test},
        "Logistic Regression" : {"predictions": y_preds_lr, "true labels": y_test},
        "KNN": {"predictions": y_preds_knn, "true labels": y_test}
        }

# make a dataframe with predictions and true labels
df = pd.DataFrame(dict)

df.head(10)

# Confusion matrix
print(f"Random Forest: \n{confusion_matrix(y_test, y_preds_rf)}")
print(f"Logistic Regression: \n{confusion_matrix(y_test, y_preds_lr)}")
print(f"KNN: \n{confusion_matrix(y_test, y_preds_knn)}")

# Make it more visual with seaborn
sns.set(font_scale = 1.5)

def plot_conf_mat(y_test, y_preds):
  """
  Plots a nice looking confusion matrix using seaborn heatmap()
  """
  fig, ax = plt.subplots(figsize=(3, 3))
  ax = sns.heatmap(confusion_matrix(y_test, y_preds),
                   annot=True,
                   cbar=False)
  plt.xlabel("True Labels")
  plt.ylabel("Predicted Labels")

print("Random Forest")
plot_conf_mat(y_test, y_preds_rf)

print("Logistic Regression")
plot_conf_mat(y_test, y_preds_lr)

print("KNN")
plot_conf_mat(y_test, y_preds_knn)

